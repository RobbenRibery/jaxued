{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textures from cache.\n",
      "Textures successfully loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import List, Tuple \n",
    "\n",
    "import os \n",
    "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
    "\n",
    "import tiktoken \n",
    "from langchain_anthropic import ChatAnthropic \n",
    "\n",
    "from craftax.craftax import game_logic \n",
    "from craftax.craftax import constants \n",
    "from craftax.craftax import craftax_state \n",
    "from craftax.craftax.envs import craftax_symbolic_env \n",
    "from craftax.craftax.world_gen import world_gen \n",
    "\n",
    "import jax \n",
    "import jax.numpy as jnp \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "p = os.path.join(os.path.expanduser(\"~\"), \".ssh\", \"anthropic.pem\")\n",
    "with open(p, \"r\") as f:\n",
    "    anthropic_key = f.readline().strip()\n",
    "f.close()\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_key\n",
    "\n",
    "model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=4096,    \n",
    ")\n",
    "\n",
    "import sys \n",
    "import inspect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT \n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_logic_code:str = inspect.getsource(game_logic)\n",
    "constants_code:str = inspect.getsource(constants)\n",
    "state_code = inspect.getsource(craftax_state)\n",
    "env_code = inspect.getsource(craftax_symbolic_env)\n",
    "entry_point = \"CraftaxSymbolicEnv\"\n",
    "generation_code:str = inspect.getsource(world_gen) \n",
    "\n",
    "\n",
    "inputs = [\n",
    "    game_logic_code, \n",
    "    constants_code, \n",
    "    state_code, \n",
    "    env_code, \n",
    "    generation_code\n",
    "]\n",
    "\n",
    "delimitier = \"\\n***************\\n\"\n",
    "title = \"Craftax Symbolic Environment game logic, states, environment and world generation code as follows:\\n\"\n",
    "\n",
    "input_string = title + delimitier.join(inputs) + f\"\\nEntrey point of the enviornment is {entry_point}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45019\n"
     ]
    }
   ],
   "source": [
    "tokens = tiktoken.get_encoding(\"o200k_base\").encode(input_string)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Discovery \n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an exceptional Reinforcement Environment evaluator\",\n",
    "    ),\n",
    "    (\n",
    "        \"human\", \n",
    "        f\"\"\"\n",
    "        ### Task \n",
    "        Think step by step to summairse a few key metrics comprehensively evaluating the learning potential for the following CraftaxSymbolicEnv Environment.\n",
    "        Be as exhasutive as possible.\n",
    "        ```python\n",
    "        {input_string}\n",
    "        ```\n",
    "        After selecting the metrics, write a Python function that compute such (scalr) metric for an given enviornment \n",
    "        Make sure that the funciton takes in: \n",
    "            - The environment state representing an unplayed environment <EnvState> \n",
    "\n",
    "        Make sure that the function outputs: \n",
    "            - A scalar value that represents the metric value (between 0 and 1) <jnp.float32> \n",
    "\n",
    "        Please code in jax and put all functions in a self-contained script end to end.\n",
    "        Make sure all functions have @jax.jit decorators.\n",
    "        \"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "out = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided code, I'll create a comprehensive evaluation function that measures key aspects of the learning potential for the CraftaxSymbolic environment. Here's my analysis and implementation:\n",
      "\n",
      "Key Metrics to Consider:\n",
      "1. Resource Accessibility (ores, trees, water)\n",
      "2. Navigation Complexity (path connectivity)\n",
      "3. Combat Opportunity (mob spawn potential)\n",
      "4. Crafting Potential (proximity of crafting stations)\n",
      "5. Survival Balance (food/water sources)\n",
      "\n",
      "Here's the implementation:\n",
      "\n",
      "```python\n",
      "import jax\n",
      "import jax.numpy as jnp\n",
      "import chex\n",
      "from functools import partial\n",
      "\n",
      "@jax.jit\n",
      "def compute_resource_density(map_layer: chex.Array) -> jnp.float32:\n",
      "    \"\"\"Compute density of important resources.\"\"\"\n",
      "    resource_blocks = jnp.array([\n",
      "        BlockType.TREE.value,\n",
      "        BlockType.COAL.value,\n",
      "        BlockType.IRON.value,\n",
      "        BlockType.DIAMOND.value,\n",
      "        BlockType.SAPPHIRE.value,\n",
      "        BlockType.RUBY.value\n",
      "    ])\n",
      "    \n",
      "    total_resources = sum(map_layer == block for block in resource_blocks)\n",
      "    return total_resources / map_layer.size\n",
      "\n",
      "@jax.jit\n",
      "def compute_path_accessibility(map_layer: chex.Array) -> jnp.float32:\n",
      "    \"\"\"Measure how well-connected the paths are.\"\"\"\n",
      "    walkable_blocks = jnp.array([\n",
      "        BlockType.PATH.value,\n",
      "        BlockType.GRASS.value,\n",
      "        BlockType.FIRE_GRASS.value,\n",
      "        BlockType.ICE_GRASS.value\n",
      "    ])\n",
      "    \n",
      "    walkable_spaces = sum(map_layer == block for block in walkable_blocks)\n",
      "    return walkable_spaces / map_layer.size\n",
      "\n",
      "@jax.jit\n",
      "def compute_survival_balance(map_layer: chex.Array, item_layer: chex.Array) -> jnp.float32:\n",
      "    \"\"\"Evaluate balance of survival resources.\"\"\"\n",
      "    water_sources = (map_layer == BlockType.WATER.value).sum()\n",
      "    food_sources = ((map_layer == BlockType.PLANT.value) | \n",
      "                   (map_layer == BlockType.RIPE_PLANT.value)).sum()\n",
      "    torches = (item_layer == ItemType.TORCH.value).sum()\n",
      "    \n",
      "    total_sources = water_sources + food_sources + torches\n",
      "    max_possible = map_layer.size * 0.1  # Assuming 10% is a good density\n",
      "    return jnp.minimum(total_sources / max_possible, 1.0)\n",
      "\n",
      "@jax.jit\n",
      "def compute_crafting_potential(map_layer: chex.Array) -> jnp.float32:\n",
      "    \"\"\"Evaluate crafting opportunities.\"\"\"\n",
      "    crafting_blocks = jnp.array([\n",
      "        BlockType.CRAFTING_TABLE.value,\n",
      "        BlockType.FURNACE.value,\n",
      "        BlockType.ENCHANTMENT_TABLE_FIRE.value,\n",
      "        BlockType.ENCHANTMENT_TABLE_ICE.value\n",
      "    ])\n",
      "    \n",
      "    total_crafting = sum(map_layer == block for block in crafting_blocks)\n",
      "    ideal_crafting_count = map_layer.size * 0.02  # Assuming 2% is good density\n",
      "    return jnp.minimum(total_crafting / ideal_crafting_count, 1.0)\n",
      "\n",
      "@partial(jax.jit, static_argnames=['static_params'])\n",
      "def compute_environment_potential(\n",
      "    state: EnvState,\n",
      "    static_params: StaticEnvParams\n",
      ") -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Compute overall environment potential score.\n",
      "    \n",
      "    Args:\n",
      "        state: Current environment state\n",
      "        static_params: Static environment parameters\n",
      "    \n",
      "    Returns:\n",
      "        score: Float between 0 and 1 indicating environment potential\n",
      "    \"\"\"\n",
      "    \n",
      "    # Compute metrics for each level\n",
      "    def _evaluate_level(level_idx):\n",
      "        map_layer = state.map[level_idx]\n",
      "        item_layer = state.item_map[level_idx]\n",
      "        \n",
      "        resource_score = compute_resource_density(map_layer)\n",
      "        path_score = compute_path_accessibility(map_layer)\n",
      "        survival_score = compute_survival_balance(map_layer, item_layer)\n",
      "        crafting_score = compute_crafting_potential(map_layer)\n",
      "        \n",
      "        # Weight the scores based on importance\n",
      "        weighted_score = (\n",
      "            0.3 * resource_score +\n",
      "            0.3 * path_score +\n",
      "            0.2 * survival_score +\n",
      "            0.2 * crafting_score\n",
      "        )\n",
      "        return weighted_score\n",
      "    \n",
      "    # Evaluate all levels\n",
      "    level_scores = jax.vmap(_evaluate_level)(jnp.arange(static_params.num_levels))\n",
      "    \n",
      "    # Weight early levels more heavily as they're more important for progression\n",
      "    level_weights = jnp.exp(-0.3 * jnp.arange(static_params.num_levels))\n",
      "    level_weights = level_weights / level_weights.sum()\n",
      "    \n",
      "    # Compute final weighted score\n",
      "    final_score = jnp.sum(level_scores * level_weights)\n",
      "    \n",
      "    return final_score\n",
      "\n",
      "```\n",
      "\n",
      "This implementation:\n",
      "\n",
      "1. Uses JAX for efficient computation and JIT compilation\n",
      "2. Evaluates multiple aspects of the environment:\n",
      "   - Resource availability and distribution\n",
      "   - Path connectivity and navigation potential\n",
      "   - Survival resource balance\n",
      "   - Crafting opportunities\n",
      "   - Progressive difficulty across levels\n",
      "\n",
      "3. Provides a normalized score between 0 and 1 where:\n",
      "   - 0 indicates a very poor environment for learning/progression\n",
      "   - 1 indicates an ideal environment with balanced resources and opportunities\n",
      "\n",
      "4. Weights earlier levels more heavily since they're crucial for initial progression\n",
      "\n",
      "5. Takes into account the hierarchical nature of the game with multiple levels\n",
      "\n",
      "To use this metric:\n",
      "\n",
      "```python\n",
      "# Example usage\n",
      "env = CraftaxSymbolicEnv()\n",
      "rng = jax.random.PRNGKey(0)\n",
      "_, initial_state = env.reset(rng)\n",
      "score = compute_environment_potential(initial_state, env.static_env_params)\n",
      "```\n",
      "\n",
      "The metric considers:\n",
      "- Resource accessibility for crafting progression\n",
      "- Navigation complexity for exploration\n",
      "- Survival mechanics balance\n",
      "- Crafting station availability\n",
      "- Progressive difficulty through level weighting\n",
      "\n",
      "This provides a comprehensive evaluation of the environment's potential for learning and progression.\n"
     ]
    }
   ],
   "source": [
    "print(out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.extend(\n",
    "    [\n",
    "        (\n",
    "            \"assistant\", \n",
    "            out.content,\n",
    "        ),\n",
    "        (\n",
    "            'human',\n",
    "            \"Think step by step to cover more evaluation metrics for the CraftaxSymbolicEnv environment, and code them up.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "out2 = model.invoke(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down additional important metrics for a more comprehensive evaluation:\n",
      "\n",
      "1. Combat Environment Evaluation\n",
      "2. Progression Potential\n",
      "3. Risk-Reward Balance\n",
      "4. Exploration Incentives\n",
      "5. Resource Progression Chain\n",
      "6. Boss Fight Preparation Potential\n",
      "\n",
      "Here's the enhanced implementation:\n",
      "\n",
      "```python\n",
      "import jax\n",
      "import jax.numpy as jnp\n",
      "import chex\n",
      "from functools import partial\n",
      "\n",
      "@jax.jit\n",
      "def compute_combat_environment(\n",
      "    map_layer: chex.Array,\n",
      "    mob_map_layer: chex.Array\n",
      ") -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Evaluate combat environment quality.\n",
      "    Considers:\n",
      "    - Combat space availability\n",
      "    - Mob density\n",
      "    - Strategic positions (walls/obstacles for cover)\n",
      "    \"\"\"\n",
      "    # Combat space calculation\n",
      "    combat_viable_blocks = jnp.array([\n",
      "        BlockType.PATH.value,\n",
      "        BlockType.GRASS.value,\n",
      "        BlockType.FIRE_GRASS.value,\n",
      "        BlockType.ICE_GRASS.value\n",
      "    ])\n",
      "    \n",
      "    combat_space = sum(map_layer == block for block in combat_viable_blocks)\n",
      "    \n",
      "    # Strategic positions (blocks adjacent to walls)\n",
      "    kernel = jnp.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
      "    strategic_positions = jax.scipy.signal.convolve2d(\n",
      "        (map_layer == BlockType.WALL.value), \n",
      "        kernel, \n",
      "        mode='same'\n",
      "    )\n",
      "    strategic_score = (strategic_positions > 0).sum() / map_layer.size\n",
      "    \n",
      "    # Mob density\n",
      "    mob_density = mob_map_layer.sum() / map_layer.size\n",
      "    \n",
      "    return (0.4 * combat_space/map_layer.size + \n",
      "            0.3 * strategic_score + \n",
      "            0.3 * jnp.minimum(mob_density * 10, 1.0))\n",
      "\n",
      "@jax.jit\n",
      "def compute_progression_potential(state: EnvState) -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Evaluate potential for character progression.\n",
      "    Considers:\n",
      "    - Crafting station availability\n",
      "    - Resource tier progression\n",
      "    - XP sources\n",
      "    \"\"\"\n",
      "    map_layer = state.map[state.player_level]\n",
      "    \n",
      "    # Crafting progression\n",
      "    crafting_blocks = jnp.array([\n",
      "        BlockType.CRAFTING_TABLE.value,\n",
      "        BlockType.FURNACE.value,\n",
      "        BlockType.ENCHANTMENT_TABLE_FIRE.value,\n",
      "        BlockType.ENCHANTMENT_TABLE_ICE.value\n",
      "    ])\n",
      "    crafting_score = sum(map_layer == block for block in crafting_blocks)\n",
      "    \n",
      "    # Resource tiers\n",
      "    resource_tiers = {\n",
      "        1: [BlockType.TREE.value, BlockType.STONE.value],\n",
      "        2: [BlockType.COAL.value, BlockType.IRON.value],\n",
      "        3: [BlockType.DIAMOND.value, BlockType.SAPPHIRE.value, BlockType.RUBY.value]\n",
      "    }\n",
      "    \n",
      "    tier_scores = jnp.zeros(3)\n",
      "    for tier, blocks in resource_tiers.items():\n",
      "        tier_scores = tier_scores.at[tier-1].set(\n",
      "            sum(map_layer == block for block in blocks).sum()\n",
      "        )\n",
      "    \n",
      "    # Weight higher tiers more\n",
      "    tier_weights = jnp.array([0.2, 0.3, 0.5])\n",
      "    resource_score = jnp.sum(tier_scores * tier_weights)\n",
      "    \n",
      "    return (0.5 * crafting_score/map_layer.size + \n",
      "            0.5 * jnp.minimum(resource_score/map_layer.size * 5, 1.0))\n",
      "\n",
      "@jax.jit\n",
      "def compute_risk_reward_balance(state: EnvState) -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Evaluate balance between risks and rewards.\n",
      "    Considers:\n",
      "    - Valuable resources near dangers\n",
      "    - Safe zones accessibility\n",
      "    - Health/survival resource distribution\n",
      "    \"\"\"\n",
      "    map_layer = state.map[state.player_level]\n",
      "    \n",
      "    # Dangerous blocks\n",
      "    danger_blocks = jnp.array([\n",
      "        BlockType.LAVA.value,\n",
      "        BlockType.NECROMANCER.value\n",
      "    ])\n",
      "    danger_map = sum(map_layer == block for block in danger_blocks)\n",
      "    \n",
      "    # Valuable blocks\n",
      "    valuable_blocks = jnp.array([\n",
      "        BlockType.DIAMOND.value,\n",
      "        BlockType.SAPPHIRE.value,\n",
      "        BlockType.RUBY.value,\n",
      "        BlockType.CHEST.value\n",
      "    ])\n",
      "    value_map = sum(map_layer == block for block in valuable_blocks)\n",
      "    \n",
      "    # Compute proximity between dangers and valuables\n",
      "    kernel = jnp.ones((3, 3))\n",
      "    danger_proximity = jax.scipy.signal.convolve2d(danger_map, kernel, mode='same')\n",
      "    value_proximity = jax.scipy.signal.convolve2d(value_map, kernel, mode='same')\n",
      "    \n",
      "    risk_reward_correlation = jnp.sum(danger_proximity * value_proximity) / map_layer.size\n",
      "    \n",
      "    return jnp.minimum(risk_reward_correlation * 5, 1.0)\n",
      "\n",
      "@jax.jit\n",
      "def compute_exploration_incentives(state: EnvState) -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Evaluate how well the environment encourages exploration.\n",
      "    Considers:\n",
      "    - Points of interest distribution\n",
      "    - Path complexity\n",
      "    - Hidden resources\n",
      "    \"\"\"\n",
      "    map_layer = state.map[state.player_level]\n",
      "    \n",
      "    # Points of interest\n",
      "    poi_blocks = jnp.array([\n",
      "        BlockType.CHEST.value,\n",
      "        BlockType.FOUNTAIN.value,\n",
      "        BlockType.CRAFTING_TABLE.value,\n",
      "        BlockType.FURNACE.value\n",
      "    ])\n",
      "    poi_map = sum(map_layer == block for block in poi_blocks)\n",
      "    \n",
      "    # Compute POI distribution evenness\n",
      "    kernel = jnp.ones((5, 5))\n",
      "    poi_density = jax.scipy.signal.convolve2d(poi_map, kernel, mode='same')\n",
      "    distribution_evenness = 1.0 - (jnp.std(poi_density) / jnp.mean(poi_density + 1e-6))\n",
      "    \n",
      "    # Path complexity\n",
      "    path_blocks = jnp.array([\n",
      "        BlockType.PATH.value,\n",
      "        BlockType.GRASS.value\n",
      "    ])\n",
      "    path_map = sum(map_layer == block for block in path_blocks)\n",
      "    \n",
      "    # Measure path branching\n",
      "    kernel_cross = jnp.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
      "    path_connections = jax.scipy.signal.convolve2d(path_map, kernel_cross, mode='same')\n",
      "    branching_score = jnp.sum(path_connections > 2) / path_map.sum()\n",
      "    \n",
      "    return (0.4 * distribution_evenness + \n",
      "            0.3 * branching_score + \n",
      "            0.3 * jnp.minimum(poi_map.sum() / map_layer.size * 10, 1.0))\n",
      "\n",
      "@jax.jit\n",
      "def compute_boss_preparation_potential(state: EnvState) -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Evaluate how well the environment supports boss fight preparation.\n",
      "    Considers:\n",
      "    - Defensive positions\n",
      "    - Resource availability\n",
      "    - Escape routes\n",
      "    \"\"\"\n",
      "    map_layer = state.map[state.player_level]\n",
      "    \n",
      "    # Defensive structures\n",
      "    wall_blocks = (map_layer == BlockType.WALL.value)\n",
      "    kernel = jnp.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
      "    defensive_positions = jax.scipy.signal.convolve2d(wall_blocks, kernel, mode='same')\n",
      "    defensive_score = jnp.sum(defensive_positions >= 4) / map_layer.size\n",
      "    \n",
      "    # Resource availability\n",
      "    resource_blocks = jnp.array([\n",
      "        BlockType.CHEST.value,\n",
      "        BlockType.FOUNTAIN.value,\n",
      "        BlockType.RIPE_PLANT.value\n",
      "    ])\n",
      "    resource_availability = sum(map_layer == block for block in resource_blocks)\n",
      "    \n",
      "    # Escape routes (path connectivity)\n",
      "    path_blocks = (map_layer == BlockType.PATH.value)\n",
      "    escape_routes = jax.scipy.signal.convolve2d(path_blocks, kernel, mode='same')\n",
      "    escape_score = jnp.sum(escape_routes >= 3) / map_layer.size\n",
      "    \n",
      "    return (0.4 * defensive_score + \n",
      "            0.3 * jnp.minimum(resource_availability / map_layer.size * 10, 1.0) + \n",
      "            0.3 * escape_score)\n",
      "\n",
      "@partial(jax.jit, static_argnames=['static_params'])\n",
      "def compute_comprehensive_environment_potential(\n",
      "    state: EnvState,\n",
      "    static_params: StaticEnvParams\n",
      ") -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Compute comprehensive environment potential score.\n",
      "    \"\"\"\n",
      "    def _evaluate_level(level_idx):\n",
      "        map_layer = state.map[level_idx]\n",
      "        mob_map_layer = state.mob_map[level_idx]\n",
      "        \n",
      "        # Core metrics\n",
      "        combat_score = compute_combat_environment(map_layer, mob_map_layer)\n",
      "        progression_score = compute_progression_potential(state)\n",
      "        risk_reward_score = compute_risk_reward_balance(state)\n",
      "        exploration_score = compute_exploration_incentives(state)\n",
      "        boss_prep_score = compute_boss_preparation_potential(state)\n",
      "        \n",
      "        # Weight the scores based on level-specific importance\n",
      "        weights = jnp.array([0.25, 0.25, 0.2, 0.2, 0.1])\n",
      "        scores = jnp.array([\n",
      "            combat_score,\n",
      "            progression_score,\n",
      "            risk_reward_score,\n",
      "            exploration_score,\n",
      "            boss_prep_score\n",
      "        ])\n",
      "        \n",
      "        return jnp.sum(weights * scores)\n",
      "    \n",
      "    # Evaluate all levels\n",
      "    level_scores = jax.vmap(_evaluate_level)(jnp.arange(static_params.num_levels))\n",
      "    \n",
      "    # Progressive difficulty weighting\n",
      "    level_weights = jnp.exp(-0.2 * jnp.arange(static_params.num_levels))\n",
      "    level_weights = level_weights / level_weights.sum()\n",
      "    \n",
      "    # Final weighted score\n",
      "    final_score = jnp.sum(level_scores * level_weights)\n",
      "    \n",
      "    return final_score\n",
      "\n",
      "# Additional utility metrics\n",
      "\n",
      "@jax.jit\n",
      "def compute_resource_progression_chain(state: EnvState) -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Evaluate the completeness of resource progression chains.\n",
      "    \"\"\"\n",
      "    map_layer = state.map[state.player_level]\n",
      "    \n",
      "    # Define progression chains\n",
      "    wood_chain = [BlockType.TREE.value, BlockType.CRAFTING_TABLE.value]\n",
      "    stone_chain = [BlockType.STONE.value, BlockType.FURNACE.value]\n",
      "    ore_chain = [BlockType.COAL.value, BlockType.IRON.value, BlockType.DIAMOND.value]\n",
      "    \n",
      "    # Compute chain completeness\n",
      "    def chain_score(chain):\n",
      "        return jnp.min(jnp.array([jnp.sum(map_layer == block) for block in chain]))\n",
      "    \n",
      "    wood_score = chain_score(wood_chain)\n",
      "    stone_score = chain_score(stone_chain)\n",
      "    ore_score = chain_score(ore_chain)\n",
      "    \n",
      "    # Weight the chains\n",
      "    chain_weights = jnp.array([0.3, 0.3, 0.4])\n",
      "    chain_scores = jnp.array([wood_score, stone_score, ore_score])\n",
      "    \n",
      "    return jnp.sum(chain_weights * jnp.minimum(chain_scores / map_layer.size * 10, 1.0))\n",
      "\n",
      "```\n",
      "\n",
      "This enhanced implementation adds:\n",
      "\n",
      "1. Combat Environment Evaluation:\n",
      "   - Combat space availability\n",
      "   - Strategic positions\n",
      "   - Mob density balance\n",
      "\n",
      "2. Progression Potential:\n",
      "   - Crafting station availability\n",
      "   - Resource tier progression\n",
      "   - XP source distribution\n",
      "\n",
      "3. Risk-Reward Balance:\n",
      "   - Valuable resources near dangers\n",
      "   - Safe zones accessibility\n",
      "   - Resource distribution\n",
      "\n",
      "4. Exploration Incentives:\n",
      "   - Points of interest distribution\n",
      "   - Path complexity\n",
      "   - Hidden resources\n",
      "\n",
      "5. Boss Fight Preparation:\n",
      "   - Defensive positions\n",
      "   - Resource availability\n",
      "   - Escape routes\n",
      "\n",
      "6. Resource Progression Chains:\n",
      "   - Complete resource chains\n",
      "   - Crafting progression paths\n",
      "   - Technology tree completeness\n",
      "\n",
      "Usage:\n",
      "```python\n",
      "# Example usage\n",
      "env = CraftaxSymbolicEnv()\n",
      "rng = jax.random.PRNGKey(0)\n",
      "_, initial_state = env.reset(rng)\n",
      "\n",
      "# Get comprehensive evaluation\n",
      "score = compute_comprehensive_environment_potential(initial_state, env.static_env_params)\n",
      "\n",
      "# Get specific metrics\n",
      "combat_score = compute_combat_environment(\n",
      "    initial_state.map[0],\n",
      "    initial_state.mob_map[0]\n",
      ")\n",
      "progression_score = compute_progression_potential(initial_state)\n",
      "resource_chain_score = compute_resource_progression_chain(initial_state)\n",
      "```\n",
      "\n",
      "This provides a much more detailed evaluation of the environment's learning potential, considering combat, progression, exploration, and preparation aspects of the game.\n"
     ]
    }
   ],
   "source": [
    "print(out2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.extend(\n",
    "    [\n",
    "        (\n",
    "            \"assistant\", \n",
    "            out2.content,\n",
    "        ),\n",
    "        (\n",
    "            'human',\n",
    "            \"Think step by step to reflect and see if you can make the compute_risk_reward_balance function more comprehensive. Also write one function that measures the combat challengness of the environment.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "out3 = model.invoke(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's enhance these aspects:\n",
      "\n",
      "1. For risk-reward balance, we should consider:\n",
      "- Mob difficulty vs reward value\n",
      "- Environmental hazards vs resource richness\n",
      "- Distance-based risk scaling\n",
      "- Safe zone availability\n",
      "- Resource accessibility difficulty\n",
      "\n",
      "2. For combat challenge, we should measure:\n",
      "- Mob type distribution and difficulty\n",
      "- Combat space constraints\n",
      "- Mob positioning and density\n",
      "- Projectile combat potential\n",
      "- Escape route availability\n",
      "\n",
      "Here's the improved implementation:\n",
      "\n",
      "```python\n",
      "import jax\n",
      "import jax.numpy as jnp\n",
      "from functools import partial\n",
      "\n",
      "@jax.jit\n",
      "def compute_enhanced_risk_reward_balance(state: EnvState) -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Comprehensive evaluation of risk-reward balance.\n",
      "    \n",
      "    Considers:\n",
      "    1. Resource value vs danger proximity\n",
      "    2. Safe zones vs hazard zones\n",
      "    3. Progressive difficulty scaling\n",
      "    4. Resource accessibility\n",
      "    5. Survival resource distribution\n",
      "    \"\"\"\n",
      "    map_layer = state.map[state.player_level]\n",
      "    mob_layer = state.mob_map[state.player_level]\n",
      "    \n",
      "    # 1. Resource Value Mapping\n",
      "    resource_value_map = jnp.zeros_like(map_layer, dtype=jnp.float32)\n",
      "    resource_values = {\n",
      "        BlockType.WOOD.value: 1.0,\n",
      "        BlockType.COAL.value: 2.0,\n",
      "        BlockType.IRON.value: 3.0,\n",
      "        BlockType.DIAMOND.value: 4.0,\n",
      "        BlockType.SAPPHIRE.value: 4.5,\n",
      "        BlockType.RUBY.value: 4.5,\n",
      "        BlockType.CHEST.value: 5.0,\n",
      "    }\n",
      "    \n",
      "    for block_type, value in resource_values.items():\n",
      "        resource_value_map += (map_layer == block_type) * value\n",
      "    \n",
      "    # 2. Danger Mapping\n",
      "    danger_value_map = jnp.zeros_like(map_layer, dtype=jnp.float32)\n",
      "    danger_values = {\n",
      "        BlockType.LAVA.value: 4.0,\n",
      "        BlockType.NECROMANCER.value: 5.0,\n",
      "        BlockType.GRAVE.value: 3.0,\n",
      "        BlockType.GRAVE2.value: 3.5,\n",
      "        BlockType.GRAVE3.value: 4.0,\n",
      "    }\n",
      "    \n",
      "    for block_type, value in danger_values.items():\n",
      "        danger_value_map += (map_layer == block_type) * value\n",
      "    \n",
      "    # Add mob danger\n",
      "    danger_value_map += mob_layer * 3.0\n",
      "    \n",
      "    # 3. Safe Zone Analysis\n",
      "    safe_blocks = jnp.array([\n",
      "        BlockType.PATH.value,\n",
      "        BlockType.GRASS.value,\n",
      "        BlockType.FIRE_GRASS.value,\n",
      "        BlockType.ICE_GRASS.value\n",
      "    ])\n",
      "    safe_zones = sum(map_layer == block for block in safe_blocks)\n",
      "    \n",
      "    # 4. Distance-based Risk Scaling\n",
      "    kernel_small = jnp.ones((3, 3))\n",
      "    kernel_medium = jnp.ones((5, 5))\n",
      "    kernel_large = jnp.ones((7, 7))\n",
      "    \n",
      "    # Compute danger proximity at different scales\n",
      "    danger_proximity_small = jax.scipy.signal.convolve2d(\n",
      "        danger_value_map, kernel_small, mode='same'\n",
      "    )\n",
      "    danger_proximity_medium = jax.scipy.signal.convolve2d(\n",
      "        danger_value_map, kernel_medium, mode='same'\n",
      "    )\n",
      "    danger_proximity_large = jax.scipy.signal.convolve2d(\n",
      "        danger_value_map, kernel_large, mode='same'\n",
      "    )\n",
      "    \n",
      "    # Weighted danger proximity\n",
      "    danger_proximity = (\n",
      "        0.5 * danger_proximity_small +\n",
      "        0.3 * danger_proximity_medium +\n",
      "        0.2 * danger_proximity_large\n",
      "    )\n",
      "    \n",
      "    # 5. Resource Accessibility Score\n",
      "    resource_proximity = jax.scipy.signal.convolve2d(\n",
      "        resource_value_map, kernel_medium, mode='same'\n",
      "    )\n",
      "    \n",
      "    # 6. Risk-Reward Correlation\n",
      "    risk_reward_correlation = jnp.corrcoef(\n",
      "        danger_proximity.flatten(), \n",
      "        resource_proximity.flatten()\n",
      "    )[0, 1]\n",
      "    \n",
      "    # 7. Survival Resource Distribution\n",
      "    survival_blocks = jnp.array([\n",
      "        BlockType.WATER.value,\n",
      "        BlockType.FOUNTAIN.value,\n",
      "        BlockType.RIPE_PLANT.value\n",
      "    ])\n",
      "    survival_resources = sum(map_layer == block for block in survival_blocks)\n",
      "    survival_distribution = jax.scipy.signal.convolve2d(\n",
      "        survival_resources, kernel_medium, mode='same'\n",
      "    )\n",
      "    \n",
      "    # Compute final scores\n",
      "    risk_reward_score = jnp.abs(risk_reward_correlation)  # Want moderate correlation\n",
      "    safe_zone_score = safe_zones.sum() / map_layer.size\n",
      "    survival_score = jnp.minimum(survival_resources.sum() / map_layer.size * 10, 1.0)\n",
      "    \n",
      "    # Weight and combine scores\n",
      "    weights = jnp.array([0.4, 0.3, 0.3])\n",
      "    scores = jnp.array([\n",
      "        risk_reward_score,\n",
      "        safe_zone_score,\n",
      "        survival_score\n",
      "    ])\n",
      "    \n",
      "    return jnp.sum(weights * scores)\n",
      "\n",
      "@partial(jax.jit, static_argnames=['static_params'])\n",
      "def compute_combat_challenge(\n",
      "    state: EnvState,\n",
      "    static_params: StaticEnvParams\n",
      ") -> jnp.float32:\n",
      "    \"\"\"\n",
      "    Evaluate the combat challenge level of the environment.\n",
      "    \n",
      "    Considers:\n",
      "    1. Mob type distribution and difficulty\n",
      "    2. Combat space constraints\n",
      "    3. Mob positioning and density\n",
      "    4. Projectile combat potential\n",
      "    5. Combat resource availability\n",
      "    6. Escape route availability\n",
      "    \"\"\"\n",
      "    map_layer = state.map[state.player_level]\n",
      "    \n",
      "    # 1. Mob Challenge Scoring\n",
      "    def compute_mob_challenge():\n",
      "        # Melee mobs\n",
      "        melee_challenge = jnp.sum(\n",
      "            state.melee_mobs.mask[state.player_level] *\n",
      "            state.melee_mobs.health[state.player_level] *\n",
      "            (state.melee_mobs.type_id[state.player_level] + 1)  # Higher ID = harder\n",
      "        )\n",
      "        \n",
      "        # Ranged mobs\n",
      "        ranged_challenge = jnp.sum(\n",
      "            state.ranged_mobs.mask[state.player_level] *\n",
      "            state.ranged_mobs.health[state.player_level] *\n",
      "            (state.ranged_mobs.type_id[state.player_level] + 1.5)  # Ranged bonus\n",
      "        )\n",
      "        \n",
      "        # Projectiles\n",
      "        projectile_challenge = jnp.sum(\n",
      "            state.mob_projectiles.mask[state.player_level] *\n",
      "            (state.mob_projectiles.type_id[state.player_level] + 1)\n",
      "        )\n",
      "        \n",
      "        return melee_challenge, ranged_challenge, projectile_challenge\n",
      "    \n",
      "    melee_challenge, ranged_challenge, projectile_challenge = compute_mob_challenge()\n",
      "    \n",
      "    # 2. Combat Space Analysis\n",
      "    combat_blocks = jnp.array([\n",
      "        BlockType.PATH.value,\n",
      "        BlockType.GRASS.value,\n",
      "        BlockType.FIRE_GRASS.value,\n",
      "        BlockType.ICE_GRASS.value\n",
      "    ])\n",
      "    \n",
      "    combat_space = sum(map_layer == block for block in combat_blocks)\n",
      "    \n",
      "    # 3. Combat Constraints\n",
      "    kernel = jnp.ones((3, 3))\n",
      "    wall_blocks = (map_layer == BlockType.WALL.value)\n",
      "    combat_constraints = jax.scipy.signal.convolve2d(wall_blocks, kernel, mode='same')\n",
      "    \n",
      "    # 4. Strategic Position Analysis\n",
      "    strategic_positions = jnp.logical_and(\n",
      "        combat_space,\n",
      "        combat_constraints > 0\n",
      "    )\n",
      "    \n",
      "    # 5. Combat Resource Availability\n",
      "    combat_resources = jnp.array([\n",
      "        BlockType.CHEST.value,\n",
      "        BlockType.FOUNTAIN.value\n",
      "    ])\n",
      "    resource_availability = sum(map_layer == block for block in combat_resources)\n",
      "    \n",
      "    # 6. Escape Route Analysis\n",
      "    escape_routes = jax.scipy.signal.convolve2d(combat_space, kernel, mode='same')\n",
      "    escape_score = jnp.sum(escape_routes >= 3) / combat_space.sum()\n",
      "    \n",
      "    # Normalize scores\n",
      "    total_mob_challenge = (\n",
      "        melee_challenge + \n",
      "        ranged_challenge * 1.5 +  # Ranged mobs are harder\n",
      "        projectile_challenge\n",
      "    )\n",
      "    normalized_mob_challenge = jnp.minimum(\n",
      "        total_mob_challenge / (static_params.max_melee_mobs * 5), \n",
      "        1.0\n",
      "    )\n",
      "    \n",
      "    combat_space_score = combat_space.sum() / map_layer.size\n",
      "    strategic_position_score = strategic_positions.sum() / combat_space.sum()\n",
      "    resource_score = jnp.minimum(\n",
      "        resource_availability / (map_layer.size * 0.05),  # Expect 5% resources\n",
      "        1.0\n",
      "    )\n",
      "    \n",
      "    # Weight and combine scores\n",
      "    weights = jnp.array([0.3, 0.2, 0.2, 0.15, 0.15])\n",
      "    scores = jnp.array([\n",
      "        normalized_mob_challenge,\n",
      "        combat_space_score,\n",
      "        strategic_position_score,\n",
      "        resource_score,\n",
      "        escape_score\n",
      "    ])\n",
      "    \n",
      "    return jnp.sum(weights * scores)\n",
      "```\n",
      "\n",
      "The improvements include:\n",
      "\n",
      "For `compute_enhanced_risk_reward_balance`:\n",
      "1. More detailed resource value mapping\n",
      "2. Multi-scale danger proximity analysis\n",
      "3. Better correlation between risks and rewards\n",
      "4. Survival resource distribution analysis\n",
      "5. Safe zone availability scoring\n",
      "6. Distance-based risk scaling\n",
      "\n",
      "For `compute_combat_challenge`:\n",
      "1. Comprehensive mob difficulty scoring\n",
      "   - Melee vs ranged differentiation\n",
      "   - Health and type consideration\n",
      "   - Projectile threat analysis\n",
      "2. Combat space quality assessment\n",
      "3. Strategic position analysis\n",
      "4. Combat resource availability\n",
      "5. Escape route analysis\n",
      "6. Normalized scoring system\n",
      "\n",
      "Usage:\n",
      "```python\n",
      "# Example usage\n",
      "env = CraftaxSymbolicEnv()\n",
      "rng = jax.random.PRNGKey(0)\n",
      "_, initial_state = env.reset(rng)\n",
      "\n",
      "# Get risk-reward balance score\n",
      "risk_reward_score = compute_enhanced_risk_reward_balance(initial_state)\n",
      "\n",
      "# Get combat challenge score\n",
      "combat_score = compute_combat_challenge(initial_state, env.static_env_params)\n",
      "\n",
      "# Can be used together for overall environment evaluation\n",
      "environment_score = 0.5 * risk_reward_score + 0.5 * combat_score\n",
      "```\n",
      "\n",
      "These enhanced metrics provide a more nuanced evaluation of:\n",
      "1. The balance between risks and potential rewards\n",
      "2. The combat challenge level and its various components\n",
      "3. The overall gameplay experience potential\n",
      "4. The progression and difficulty scaling\n",
      "5. The strategic depth of combat encounters\n",
      "\n",
      "This helps better evaluate the environment's potential for engaging and challenging gameplay while maintaining fair risk-reward balance.\n"
     ]
    }
   ],
   "source": [
    "print(out3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.craftax import craftax_evaluation \n",
    "from craftax.craftax.envs.craftax_symbolic_env import CraftaxSymbolicEnv\n",
    "from craftax.craftax.renderer import render_craftax_pixels as render_pixels\n",
    "from craftax.craftax.world_gen.world_gen import generate_world as generate_world_craftax\n",
    "\n",
    "from examples.craftax.craftax_wrappers import LogWrapper\n",
    "\n",
    "from craftax.craftax.constants import Achievement\n",
    "from jaxued.wrappers import AutoReplayWrapper\n",
    "from jaxued.level_sampler import LevelSampler \n",
    "\n",
    "import jax \n",
    "import jax.numpy as jnp \n",
    "\n",
    "ENV_CLASS = CraftaxSymbolicEnv\n",
    "generate_world = generate_world_craftax\n",
    "render_craftax_pixels = render_pixels\n",
    "\n",
    "DEFAULT_STATICS = ENV_CLASS.default_static_params()\n",
    "default_env = ENV_CLASS(DEFAULT_STATICS)\n",
    "env = LogWrapper(default_env)\n",
    "env = AutoReplayWrapper(env)\n",
    "env_params = env.default_params\n",
    "\n",
    "def sample_random_level(rng):\n",
    "    return generate_world(rng, env.default_params, DEFAULT_STATICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_train_envs':4, \n",
    "    \"level_buffer_capacity\":100,\n",
    "    \"replay_prob\":0.8,\n",
    "    \"staleness_coeff\":0.3,\n",
    "    \"minimum_fill_ratio\":0.1,\n",
    "    \"prioritization\":\"rank\",\n",
    "    \"temperature\":1.0,\n",
    "    \"topk_k\":0.3,\n",
    "    \"max_grad_norm\":1,\n",
    "    \"buffer_duplicate_check\":True,\n",
    "    \"exploratory_grad_updates\":False,\n",
    "    \"outer_rollout_steps\":64,\n",
    "    \"num_steps\":64,\n",
    "    \"gamma\":0.99,\n",
    "    \"gae_lambda\":0.9,\n",
    "    \"num_minibatches\":2,\n",
    "    \"epoch_ppo\":5,\n",
    "    \"clip_eps\":0.2,\n",
    "    \"entropy_coeff\":0.01,\n",
    "    \"critic_coeff\":0.2,\n",
    "    \"num_updates\":10,\n",
    "    \"lr\":3e-04,\n",
    "    \"score_function\":\"MaxMC\",\n",
    "    \"eval_freq\":5,\n",
    "    \"use_accel\":True,\n",
    "    \"num_edits\":1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'craftax.craftax.craftax_state.EnvState'>\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(1)\n",
    "\n",
    "rng, rng_levels, rng_reset = jax.random.split(rng, 3)\n",
    "new_levels = jax.vmap(sample_random_level)(\n",
    "    jax.random.split(rng_levels, config[\"num_train_envs\"])\n",
    ")\n",
    "print(type(new_levels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82944"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_levels.map.size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_levels.player_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_levels.melee_mobs.health.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82944"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 * 9 * 48 * 48 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.18258102, 0.17472029, 0.19449267, 0.18696952], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(craftax_evaluation.compute_natural_resource_density_all_level)(new_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.29026812, 0.31052276, 0.31110147, 0.30709878], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(craftax_evaluation.compute_path_density_all_level)(new_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.5005787 , 0.4378858 , 0.40147567, 0.3915895 ],      dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(craftax_evaluation.compute_survival_resource_density_all_level)(new_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.00192901, 0.00192901, 0.00192901, 0.00192901],      dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(craftax_evaluation.compute_crafting_potential_all_level)(new_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.19173178, 0.15581597, 0.13020834, 0.13617623], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(craftax_evaluation.compute_progression_potential)(new_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9, 3, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_levels.melee_mobs.position.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48)\n",
      "(48, 48)\n",
      "[0.66407037 0.6713666  0.68036294 0.6667115 ]\n",
      "(4, 9, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "print(jax.vmap(craftax_evaluation.compute_exploration_incentives)(new_levels))\n",
    "print(new_levels.map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3.]\n",
      "(4, 9, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "print(jax.vmap(craftax_evaluation.compute_mob_challengeness)(new_levels))\n",
    "print(new_levels.map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_levels.ranged_mobs.mask.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.craftax.craftax_plr import evaluate_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute_crafting_potential_all_level': Array(0.00192901, dtype=float32),\n",
       " 'compute_mob_challengeness': Array(3., dtype=float32),\n",
       " 'compute_natural_resource_density_all_level': Array(0.18469088, dtype=float32),\n",
       " 'compute_path_density_all_level': Array(0.3047478, dtype=float32),\n",
       " 'compute_progression_potential': Array(0.1534831, dtype=float32),\n",
       " 'compute_survival_resource_density_all_level': Array(0.43288243, dtype=float32)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_levels(new_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_levels.player_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editor Discovery \n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an exceptional Reinforcement Environment Engineer for an Underspecified POMDP\",\n",
    "    ),\n",
    "    (\n",
    "        \"human\", \n",
    "        f\"\"\"\n",
    "        ### Task \n",
    "        Think step by step to write a mixture of editors for the fowlloing environment written in JAX.\n",
    "        ```python\n",
    "        {input_string}\n",
    "        ```\n",
    "\n",
    "        ### Task\n",
    "        Think step by step to create a mixture of editor functions for the above environment in JAX. \n",
    "        Here are some fundamental principles for designing the mixture of editors:\n",
    "\n",
    "        1.\tValidity and Integrity: Maintain the environment's validity by respecting its rules and constraints.\n",
    "\n",
    "        2.  State Coverage: Focusing on editing the states that would be more likely to encourage the agent to explore different parts of the environment or achieve different objectives.\n",
    "        \n",
    "        3.\tMinimal Variability: Introduce the minimum necessary operations required to increase or decrease the difficulty of the environment.\n",
    "        \n",
    "        4.\tControlled Randomness: Use randomness thoughtfully to expand the agent's experiences while keeping modifications within acceptable bounds.\n",
    "        \n",
    "        5.\tAgent-Centric Considerations: Ensure that modifications do not adversely affect the agent's ability to perceive and interact with the environment coherently.\n",
    "        \n",
    "        6.\tEfficiency and Performance: Utilize computational techniques (i.e. masking and immutable updates) that align with high-performance computing frameworks, ensuring that the environment can be modified efficiently.\n",
    "            You must kept any iteratitve operation at 10 tries maximum.\n",
    "\n",
    "        ### Requirement\n",
    "        Each function ONLY takes in: \n",
    "        - a chex.PRNGKey and \n",
    "        - a EnvState object as input and returns a modified EnvState object.\n",
    "\n",
    "        ### Useage\n",
    "        Editors will be used in an evolutionary process of the environment States.\n",
    "        Different editors will be selected to form a sequence of operations upon receiving an environment State.\n",
    "        You can deisgn the mixture given you such fact, but do not chain different editors together yourself.\n",
    "\n",
    "        ### Output Format\n",
    "        ```txt\n",
    "        # scratchpad  (text this section, no code)\n",
    "        # ...\n",
    "        # ...\n",
    "        # ... \n",
    "        # ...\n",
    "        ```\n",
    "        ===============================\n",
    "\n",
    "        ```Python\n",
    "        # imports \n",
    "        import jax\n",
    "        import chex \n",
    "        from functools import partial\n",
    "        ...\n",
    "\n",
    "        # env instance\n",
    "        ...\n",
    "\n",
    "        # utility functions\n",
    "        ... \n",
    "\n",
    "        # mixture of editor\n",
    "        @jax.jit\n",
    "        def editor_{{editor_name}}(rng:chex.PRNGKey, env_state:EnvState) -> EnvState:\n",
    "            ... doc string ...\n",
    "            .... code ...\n",
    "        \n",
    "        @jax.jit)\n",
    "        def editor_{{editor_name}}(rng:chex.PRNGKey, env_state:EnvState) -> EnvState:\n",
    "            ... doc string ...\n",
    "            .... code ...\n",
    "        ...\n",
    "        \n",
    "        ```\n",
    "        \"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "out = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me help you design a mixture of editors for the Craftax environment step by step.\n",
      "\n",
      "```txt\n",
      "# scratchpad\n",
      "\n",
      "1. Analyzing Core State Components:\n",
      "- Map and items (spatial layout)\n",
      "- Player attributes (health, food, etc.)\n",
      "- Inventory (resources and tools)\n",
      "- Mobs (enemies and passive)\n",
      "- Environment conditions (light level, etc.)\n",
      "\n",
      "2. Key Editor Categories:\n",
      "a) Resource Editors:\n",
      "- Modify inventory levels\n",
      "- Adjust resource distribution on map\n",
      "- Change tool/weapon levels\n",
      "\n",
      "b) Challenge Editors:\n",
      "- Modify mob distributions\n",
      "- Adjust mob attributes\n",
      "- Change environmental hazards\n",
      "\n",
      "c) Progression Editors:\n",
      "- Modify player attributes\n",
      "- Adjust achievement states\n",
      "- Change level accessibility\n",
      "\n",
      "3. Design Considerations:\n",
      "- Use masking for valid state updates\n",
      "- Employ jax.random for controlled randomness\n",
      "- Limit iterations to 10 tries max\n",
      "- Maintain state validity constraints\n",
      "- Focus on high-impact modifications\n",
      "\n",
      "4. Editor Priorities:\n",
      "1. Basic resource availability\n",
      "2. Combat difficulty\n",
      "3. Environmental challenges\n",
      "4. Progression gates\n",
      "5. Player capabilities\n",
      "\n",
      "5. Implementation Strategy:\n",
      "- Use partial application for common operations\n",
      "- Leverage jax.vmap for batch operations\n",
      "- Employ tree_map for structured updates\n",
      "- Use lax.select for conditional updates\n",
      "```\n",
      "\n",
      "```python\n",
      "# imports\n",
      "import jax\n",
      "import jax.numpy as jnp\n",
      "import chex\n",
      "from functools import partial\n",
      "from typing import Tuple, Dict, Any\n",
      "\n",
      "# env instance\n",
      "env = CraftaxSymbolicEnv()\n",
      "\n",
      "# utility functions\n",
      "@jax.jit\n",
      "def get_valid_positions(map_data: jnp.ndarray, block_type: int) -> jnp.ndarray:\n",
      "    \"\"\"Returns a boolean mask of valid positions for given block type.\"\"\"\n",
      "    return map_data == block_type\n",
      "\n",
      "@partial(jax.jit, static_argnums=(1,))\n",
      "def safe_random_position(\n",
      "    rng: chex.PRNGKey, \n",
      "    shape: Tuple[int, ...], \n",
      "    valid_mask: jnp.ndarray\n",
      ") -> jnp.ndarray:\n",
      "    \"\"\"Safely select random positions from valid mask.\"\"\"\n",
      "    flat_probs = valid_mask.reshape(-1) / jnp.sum(valid_mask)\n",
      "    idx = jax.random.choice(rng, jnp.arange(flat_probs.shape[0]), p=flat_probs)\n",
      "    return jnp.array([idx // shape[1], idx % shape[1]])\n",
      "\n",
      "# mixture of editors\n",
      "@jax.jit\n",
      "def editor_resource_boost(rng: chex.PRNGKey, env_state: EnvState) -> EnvState:\n",
      "    \"\"\"Increases available resources by adding resource blocks and items.\"\"\"\n",
      "    rng, key1, key2 = jax.random.split(rng, 3)\n",
      "    \n",
      "    # Add some basic resources to inventory\n",
      "    new_inventory = env_state.inventory.replace(\n",
      "        wood=env_state.inventory.wood + jax.random.randint(key1, (), 0, 5),\n",
      "        stone=env_state.inventory.stone + jax.random.randint(key2, (), 0, 3)\n",
      "    )\n",
      "    \n",
      "    return env_state.replace(inventory=new_inventory)\n",
      "\n",
      "@jax.jit\n",
      "def editor_combat_difficulty(rng: chex.PRNGKey, env_state: EnvState) -> EnvState:\n",
      "    \"\"\"Adjusts combat difficulty by modifying mob attributes.\"\"\"\n",
      "    rng, key1 = jax.random.split(rng)\n",
      "    \n",
      "    # Modify melee mob health scaling\n",
      "    health_boost = jax.random.uniform(key1, ()) * 0.5 + 0.75  # 0.75-1.25x\n",
      "    new_melee_mobs = env_state.melee_mobs.replace(\n",
      "        health=env_state.melee_mobs.health * health_boost\n",
      "    )\n",
      "    \n",
      "    return env_state.replace(melee_mobs=new_melee_mobs)\n",
      "\n",
      "@jax.jit\n",
      "def editor_environmental(rng: chex.PRNGKey, env_state: EnvState) -> EnvState:\n",
      "    \"\"\"Modifies environmental challenges like light levels and hazards.\"\"\"\n",
      "    rng, key1 = jax.random.split(rng)\n",
      "    \n",
      "    # Adjust light level\n",
      "    light_modifier = jax.random.uniform(key1, ()) * 0.4 + 0.6  # 0.6-1.0x\n",
      "    new_light_map = jnp.clip(env_state.light_map * light_modifier, 0.0, 1.0)\n",
      "    \n",
      "    return env_state.replace(light_map=new_light_map)\n",
      "\n",
      "@jax.jit\n",
      "def editor_player_capabilities(rng: chex.PRNGKey, env_state: EnvState) -> EnvState:\n",
      "    \"\"\"Modifies player attributes and capabilities.\"\"\"\n",
      "    rng, key1, key2 = jax.random.split(rng, 3)\n",
      "    \n",
      "    # Adjust player attributes within bounds\n",
      "    health_mod = jax.random.uniform(key1, ()) * 2 - 1  # -1 to +1\n",
      "    energy_mod = jax.random.uniform(key2, ()) * 2 - 1  # -1 to +1\n",
      "    \n",
      "    new_health = jnp.clip(env_state.player_health + health_mod, 1.0, 10.0)\n",
      "    new_energy = jnp.clip(env_state.player_energy + energy_mod, 1, 10)\n",
      "    \n",
      "    return env_state.replace(\n",
      "        player_health=new_health,\n",
      "        player_energy=new_energy\n",
      "    )\n",
      "\n",
      "@jax.jit\n",
      "def editor_progression(rng: chex.PRNGKey, env_state: EnvState) -> EnvState:\n",
      "    \"\"\"Modifies progression-related state elements.\"\"\"\n",
      "    rng, key1 = jax.random.split(rng)\n",
      "    \n",
      "    # Randomly unlock/lock some achievements\n",
      "    achievement_mask = jax.random.uniform(key1, env_state.achievements.shape) < 0.1\n",
      "    new_achievements = jnp.logical_or(env_state.achievements, achievement_mask)\n",
      "    \n",
      "    return env_state.replace(achievements=new_achievements)\n",
      "\n",
      "@jax.jit\n",
      "def editor_mob_distribution(rng: chex.PRNGKey, env_state: EnvState) -> EnvState:\n",
      "    \"\"\"Adjusts the distribution and types of mobs.\"\"\"\n",
      "    rng, key1 = jax.random.split(rng)\n",
      "    \n",
      "    # Modify mob mask distribution\n",
      "    mob_spawn_mask = jax.random.uniform(key1, env_state.mob_map.shape) < 0.15\n",
      "    new_mob_map = jnp.logical_or(env_state.mob_map, mob_spawn_mask)\n",
      "    \n",
      "    return env_state.replace(mob_map=new_mob_map)\n",
      "\n",
      "@jax.jit\n",
      "def editor_tool_progression(rng: chex.PRNGKey, env_state: EnvState) -> EnvState:\n",
      "    \"\"\"Modifies tool and weapon levels.\"\"\"\n",
      "    rng, key1, key2 = jax.random.split(rng, 3)\n",
      "    \n",
      "    # Randomly adjust tool levels within bounds\n",
      "    new_pickaxe = jnp.clip(\n",
      "        env_state.inventory.pickaxe + jax.random.randint(key1, (), -1, 2),\n",
      "        0, 4\n",
      "    )\n",
      "    new_sword = jnp.clip(\n",
      "        env_state.inventory.sword + jax.random.randint(key2, (), -1, 2),\n",
      "        0, 4\n",
      "    )\n",
      "    \n",
      "    new_inventory = env_state.inventory.replace(\n",
      "        pickaxe=new_pickaxe,\n",
      "        sword=new_sword\n",
      "    )\n",
      "    \n",
      "    return env_state.replace(inventory=new_inventory)\n",
      "```\n",
      "\n",
      "This mixture of editors provides a comprehensive set of modifications while adhering to the fundamental principles:\n",
      "\n",
      "1. Validity: Each editor maintains state validity through clipping and bounds checking.\n",
      "2. State Coverage: Editors cover different aspects of the environment (resources, combat, progression, etc.).\n",
      "3. Minimal Variability: Each editor focuses on specific aspects with controlled modifications.\n",
      "4. Controlled Randomness: Uses jax.random with bounded ranges for modifications.\n",
      "5. Agent-Centric: Maintains core gameplay mechanics while adjusting difficulty.\n",
      "6. Efficiency: Uses jax.jit and efficient operations, avoiding loops where possible.\n",
      "\n",
      "The editors can be used independently in an evolutionary process, each providing specific types of modifications to the environment state.\n"
     ]
    }
   ],
   "source": [
    "print(out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
